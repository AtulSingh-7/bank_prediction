# -*- coding: utf-8 -*-
"""Customer_Subscription.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZrulQxODstjTxwkhwJjXbL5SX9mMA7R

#  BANK MARKETING DATASET

 [URL FOR DATASET](https://archive.ics.uci.edu/dataset/222/bank+marketing)

# DOWNLOADING DATASET
"""

!wget "https://archive.ics.uci.edu/static/public/222/bank+marketing.zip"

#Unzip the zip files
!unzip "bank+marketing.zip"
!unzip "bank-additional.zip"

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score

"""# CLEANING THE DATASET

"""

#load data  to dataframe
df1 = pd.DataFrame(pd.read_csv("bank-additional/bank-additional-full.csv",sep=';'))

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df1.head()

df1.info()

# All unique values in a cloumn
for col in df1.columns:
  print( "Column Name : -", col ,df1[col].unique())
  print(20*'------')

# Number of Missing values in each Column
print(" Missing Values Count ")
for col in df1.columns:
  print(col,"  ---> ",df1[df1[col] == 'unknown'][col].count())
  print(20*'-----')

# Drop the instances with missing values

df1 = df1[df1['job'] != 'unknown']
df1 = df1[df1['marital'] != 'unknown']
df1 = df1[df1['loan'] != 'unknown']
df1 = df1[df1['housing'] != 'unknown']
df1 = df1[df1['education'] != 'unknown']
df1 = df1[df1['default'] != 'unknown']

df1.info()

"""# OUTLIER DETECTION AND REMOVAL"""

# Change Name of columns to Proper Format

df1.rename(columns={'emp.var.rate': 'emp_var_rate'}, inplace=True)
df1.rename(columns={'cons.price.idx': 'cons_price_idx'}, inplace=True)
df1.rename(columns={'nr.employed': 'nr_employed'}, inplace=True)
df1.rename(columns={'cons.conf.idx':'cons_conf_idx'}, inplace=True)

# Using Inner Quartile Range For Outlier Detection
Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

"""PLOTING HISTOGRAM FOR NUMERICAL DATA AND COUNTPLOT FOR CATEGORICAL DATA"""

sns.histplot(data=df1, x="age",kde=True)

sns.histplot(data=df1, x="duration",kde=True)

sns.histplot(data=df1, x="campaign",kde=True)

sns.histplot(data=df1, x="pdays",kde=True)

sns.histplot(data=df1, x="previous",kde=True)

sns.histplot(data=df1, x="emp_var_rate",kde=True)

sns.histplot(data=df1, x="cons_price_idx",kde=True)

sns.histplot(data=df1, x="cons_conf_idx",kde=True)

sns.histplot(data=df1, x="euribor3m",kde=True)

sns.histplot(data=df1, x="nr_employed",kde=True)

plt.figure(figsize=(15, 6))
sns.countplot(data=df1,x='job')

sns.countplot(data=df1,x='marital')

plt.figure(figsize=(15, 6))
sns.countplot(data=df1,x='education')

sns.countplot(data=df1,x='default')

sns.countplot(data=df1,x='contact')

sns.countplot(data=df1,x='housing')

sns.countplot(data=df1,x='loan')

sns.countplot(data=df1,x='month')

sns.countplot(data=df1,x='day_of_week')

sns.countplot(data=df1,x='poutcome')

"""OUTLIER REMOVING

Only for the age , cons_conf_idx column because they have a very small proportion of the dataset and droping the default column due to its imbalanced nature.

"""

df1.drop(df1[(df1['age'] < (Q1.age - 1.5 * IQR.age)) |(df1['age'] > (Q3.age + 1.5 * IQR.age))].index,inplace=True)
df1.drop(df1[(df1['cons_conf_idx'] < (Q1.cons_conf_idx - 1.5 * IQR.cons_conf_idx)) |(df1['cons_conf_idx'] > (Q3.cons_conf_idx + 1.5 * IQR.cons_conf_idx))].index,inplace=True)
df1=df1.drop('default',axis=1)

df1.info()

"""# DATA PREPROCESSING"""

# Encoding the Categorical Values

label_encoder = LabelEncoder()
df1['marital'] = label_encoder.fit_transform(df1['marital'])
df1['education'] = label_encoder.fit_transform(df1['education'])
df1['contact'] = label_encoder.fit_transform(df1['contact'])
df1['housing'] = label_encoder.fit_transform(df1['housing'])
df1['loan'] = label_encoder.fit_transform(df1['loan'])
df1['day_of_week'] = label_encoder.fit_transform(df1['day_of_week'])
df1['month'] = label_encoder.fit_transform(df1['month'])
df1['job'] = label_encoder.fit_transform(df1['job'])
df1['poutcome'] = label_encoder.fit_transform(df1['poutcome'])
df1['y'] = label_encoder.fit_transform(df1['y'])

df1.head()

"""#  Exploratory Data Analysis (EDA)"""

# Distribution Plot of Age
sns.histplot(df1['age'])

#Correlation of all features with Subscription
df1.corr()['y'][:-1]

# Plotting the heat map for Correlation matrix

plt.figure(figsize=(10,10))
sns.heatmap(df1.corr(), annot=True, cmap='coolwarm',fmt="0.1f")

"""# Machine Learning Algoirithms


*   Logistic Regression

*    Decision Tree

*    Random Forest

Splitting Data
"""

#Split the Data for Training and Testing

X = df1.drop('y', axis=1)
y = df1['y']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

print("Size of X_train:", X_train.shape)
print("Size of X_test:", X_test.shape)

"""Logistic Regression Model"""

# Import Loistic Regression

logreg = LogisticRegression()

#Training the model
logreg.fit(X_train, y_train)

# Check accuracy , recall , precision score, f-1 score

y_pred = logreg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(" F1 Score (Logistic Regression) :- ", f1)
print(" Accuracy Score (Logistic Regression) :- ",accuracy)
print(" Recall Score (Logistic Regression) :- ",recall)
print(" Precision Score (Logistic Regression) :- ",precision)

# Calculating weights of each feature
weights = logreg.coef_[0] * (X.max(axis=0) - X.min(axis=0)) / X.std(axis=0)

# Arrange weights in descending order magnitude wise
sorted_weights = sorted(zip(X.columns, weights), key=lambda x: abs(x[1]), reverse=True)
# Print the sorted weights only two decimals
for feature, weight in sorted_weights:
  print(f"{feature}: {weight:.4f}")
  print(20*"-----")

"""Decision Tree Model"""

# Import Decision Tree Classifier

dtc = DecisionTreeClassifier()

#Training the Model

dtc.fit(X_train, y_train)

# Check accuracy, recall, precision score

y_pred_dtc = dtc.predict(X_test)
accuracy_dtc = accuracy_score(y_test, y_pred_dtc)
recall_dtc = recall_score(y_test, y_pred_dtc)
precision_dtc = precision_score(y_test, y_pred_dtc)
print("Accuracy Score (Decision Tree Classifier) : -", accuracy_dtc)
print("Recall Score (Decision Tree Classifier) : -", recall_dtc)
print("Precision Score (Decision Tree Classifier) : -", precision_dtc)

"""Random forest"""

# Import Random Tree Classifier

rfc = RandomForestClassifier()

#Training the Model

rfc.fit(X_train, y_train)

# Check accuracy, recall, precision score

y_pred_rfc = rfc.predict(X_test)
accuracy_rfc = accuracy_score(y_test, y_pred_rfc)
recall_rfc = recall_score(y_test, y_pred_rfc)
precision_rfc = precision_score(y_test, y_pred_rfc)
print("Accuracy Score (Random Forest Classifier) : -", accuracy_rfc)
print("Recall Score (Random Forest Classifier) : -", recall_rfc)
print("Precision Score (Random Forest Classifier) : -", precision_rfc)

"""**CONCLUSION**
---
---




*   The dataset belongs to a particular location and type of people  of that
    area and thier response to the bank .
*   It has a lot of missing Values and outliers so need to be processed accurately .

*   It can be used to study finance of people of Portuguese and help the bank to improve its communication with customers and also increase its worth by predicting which customers would be more likely to take subscription and benefit hte bank.




"""